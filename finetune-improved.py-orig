import argparse
import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, default_data_collator
from datasets import load_from_disk
from peft import LoraConfig, get_peft_model
from transformers import DataCollatorForLanguageModeling

def parse_args():
    parser = argparse.ArgumentParser(description="Fine-tune model with optional Unsloth support.")

    # Model & dataset paths
    parser.add_argument("--model-dir", type=str, required=True, help="Directory containing the base model")
    parser.add_argument("--dataset-dir", type=str, required=True, help="Directory of the processed dataset")
    parser.add_argument("--output-dir", type=str, required=True, help="Directory to save the fine-tuned model")

    # Training params
    parser.add_argument("--num-train-epochs", type=int, default=3, help="Number of training epochs (default: %(default)s)")
    parser.add_argument("--per-device-train-batch-size", type=int, default=2, help="Training batch size per device (default: %(default)s)")
    parser.add_argument("--gradient-accumulation-steps", type=int, default=1, help="Number of gradient accumulation steps (default: %(default)s)")

    # Subset training control
    parser.add_argument("--subset-ratio", type=float, default=1.0, help="Fraction of dataset to use (default: %(default)s)")

    # Tokenization & training behavior
    parser.add_argument("--max-length", type=int, default=1120, help="Max token length (default: %(default)s)")
    parser.add_argument("--save-steps", type=int, default=2000, help="Checkpoint save interval (default: %(default)s)")
    parser.add_argument("--logging-steps", type=int, default=100, help="Logging interval (default: %(default)s)")

    # Enable Unsloth
    parser.add_argument("--use-unsloth", action="store_true", help="Enable Unsloth for faster training")

    return parser.parse_args()

def print_training_summary(args):
    """Prints a boxed summary of the training configuration."""
    summary = [
        ("Model", args.model_dir),
        ("Dataset", args.dataset_dir),
        ("Subset Ratio", f"{args.subset_ratio} (Full Dataset)" if args.subset_ratio == 1.0 else f"{args.subset_ratio}"),
        ("Max Token Length", args.max_length),
        ("Batch Size", args.per_device_train_batch_size),
        ("Gradient Steps", args.gradient_accumulation_steps),
        ("Epochs", args.num_train_epochs),
        ("Save Steps", args.save_steps),
        ("Logging Steps", args.logging_steps),
        ("Using Unsloth", "Yes" if args.use_unsloth else "No"),
    ]
    
    # Determine box width
    box_width = max(len(f"{label}: {value}") for label, value in summary) + 4

    # Print boxed summary
    print("+" + "-" * (box_width) + "+")
    print(f"|{'TRAINING SUMMARY'.center(box_width)}|")
    print("+" + "-" * (box_width) + "+")
    
    for label, value in summary:
        print(f"| {label.ljust(20)} {str(value).ljust(box_width - 24)} |")

    print("+" + "-" * (box_width) + "+")

def preprocess_function(example, tokenizer, max_length=1024):
    text = example.get("text", "")
    return tokenizer(text, truncation=True, max_length=max_length)

def load_model(args):
    if args.use_unsloth:
        return load_unsloth_model(args)
    else:
        return load_peft_model(args)

def load_unsloth_model(args):
    if not torch.cuda.is_available():
        raise RuntimeError("Unsloth requires CUDA, but no GPU was detected.")
    
    from unsloth import FastLanguageModel
    import unsloth.models._utils

    # Patch `_unsloth_get_batch_samples`
    import inspect
    def patched_get_batch_samples(self, epoch_iterator, num_batches):
        batch_samples = []
        num_items_in_batch = None

        # Check if model allows **kwargs
        model = self.model
        #NIALLDEBUG
        f = getattr(model.base_model, "model", model).forward
        #f = model.base_model.model.forward if hasattr(model, "base_model") else model.forward
        has_kwargs = tuple(inspect.signature(f).parameters.values())[-1].kind == inspect._VAR_KEYWORD

        # Iterate to find all batches
        for _ in range(num_batches):
            try:
                batch_samples += [next(epoch_iterator)]
            except StopIteration:
                break
        pass
        # Get num_items_in_batch
        if has_kwargs and len(batch_samples) > 0 and "labels" in batch_samples[0]:
            try:
                num_items_in_batch = sum(
                    [(x["labels"][..., 1:] != -100).sum() for x in batch_samples]
                )

                if self.args.average_tokens_across_devices:
                    num_items_in_batch = self.accelerator.gather(num_items_in_batch).sum().item()

                if torch.is_tensor(num_items_in_batch):
                    num_items_in_batch = num_items_in_batch.item()

            except Exception as exception:
                logger.warning_once(exception)
        pass

        return batch_samples, num_items_in_batch
    pass


    # Apply the patch
    unsloth.models._utils._unsloth_get_batch_samples = patched_get_batch_samples

    print("Using Unsloth for faster fine-tuning")
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=args.model_dir,
        max_seq_length=2048,
        dtype=torch.bfloat16,
        load_in_4bit=False,
        load_in_8bit=False
    )
        
    # Do not compile Unsloth models since they are quantized  # Enable torch.compile() for speed
    return model, tokenizer

import os
from datasets import load_from_disk, load_dataset

def load_or_tokenize_dataset(dataset_dir, tokenizer, subset_ratio=1.0, max_length=1120):
    """Loads a pre-tokenized dataset if available, otherwise tokenizes and saves it."""

    tokenized_path = os.path.join(dataset_dir, "tokenized")

    # Check if pre-tokenized dataset exists
    if os.path.exists(tokenized_path):
        print(f"ðŸ”¹ Loading pre-tokenized dataset from {tokenized_path}")
        dataset = load_from_disk(tokenized_path)
    else:
        print(f"âš¡ Tokenizing dataset (this may take a while)...")

        # Load dataset from Arrow or JSON
        if os.path.isdir(dataset_dir) and "train" in os.listdir(dataset_dir):  
            dataset = load_from_disk(dataset_dir)["train"]  # Load Arrow format
        else:
            dataset = load_dataset("json", data_files=os.path.join(dataset_dir, "data.json"))["train"]  # Load JSON

        # Tokenization function
        def preprocess_function(examples):
            return tokenizer(examples["text"], truncation=True, max_length=max_length, padding="max_length")

        # Apply tokenization
        dataset = dataset.map(preprocess_function, batched=True)

        # Save tokenized dataset
        dataset.save_to_disk(tokenized_path)
        print(f"âœ… Tokenized dataset saved to {tokenized_path}")

    # Handle subset selection
    if subset_ratio < 1.0:
        subset_size = int(subset_ratio * len(dataset))
        dataset = dataset.select(range(subset_size))
        print(f"ðŸ”¹ Using subset: {subset_size} samples")

    return dataset

def load_peft_model(args):
    print("Using PEFT for fine-tuning")
    tokenizer = AutoTokenizer.from_pretrained(args.model_dir)
    model = AutoModelForCausalLM.from_pretrained(args.model_dir)
    model = model.to(torch.device("cuda" if torch.cuda.is_available() else "cpu"))
    
    lora_config = LoraConfig(
        r=4,
        lora_alpha=32,
        target_modules=["q_proj", "v_proj"],
        lora_dropout=0.1,
        bias="none",
        task_type="CAUSAL_LM"
    )
    model = get_peft_model(model, lora_config)
    return model, tokenizer


def main():
    args = parse_args()

    # Load model and tokenizer
    model, tokenizer = load_model(args)
    train_dataset = load_or_tokenize_dataset(args.dataset_dir, tokenizer, args.subset_ratio, args.max_length)

    full_dataset = load_from_disk(args.dataset_dir)
    full_train_dataset = full_dataset["train"] if "train" in full_dataset else full_dataset

    def preprocess_function(examples, tokenizer):
        return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=512)

    # Apply pre-tokenization
    #tokenized_dataset = full_dataset.map(lambda x: preprocess_function(x, tokenizer), batched=True)

    # Save the tokenized dataset for reuse
    #tokenized_dataset.save_to_disk("tokenized_alpaca")
    print("Dataset tokenized and saved successfully!")

    #subset_size = int(0.2 * len(full_train_dataset))  # Use 1% of dataset for testing
    #subset_dataset = full_train_dataset.shuffle(seed=42).select(range(subset_size))
    #train_dataset = subset_dataset.map(lambda ex: preprocess_function(ex, tokenizer), batched=True)

    dataset=load_from_disk("tokenized_alpaca")
    train_dataset = dataset["train"] if "train" in dataset else dataset

    print(f"Full dataset size: {len(dataset)}, Training dataset size: {len(train_dataset)}")
    # Create data collator
    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

    # Set up training arguments
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        per_device_train_batch_size=args.per_device_train_batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        num_train_epochs=args.num_train_epochs,
        fp16=False,  # BF16 is preferred over FP16
        bf16=torch.cuda.is_available(),  # Enable BF16 when CUDA is available
        logging_steps=100,
        save_steps=2000,
        save_total_limit=1,
        evaluation_strategy="no",
        report_to="none",
        gradient_checkpointing=True
    )

    # Initialize Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        data_collator=data_collator,
        tokenizer=tokenizer
    )

    # Start training
    trainer.train()

    # Save the fine-tuned model
    trainer.save_model(args.output_dir)
    print(f"Training complete. Model saved to {args.output_dir}")

if __name__ == "__main__":
    main()
